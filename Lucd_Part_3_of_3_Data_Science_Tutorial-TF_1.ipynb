{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/Capture2.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <center> Tutorial: Data Science Tutorial Part 3 of 3 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background on Lucd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lucd Enterprise AI Data Science Platform is a highly secure, scalable, open and flexible platform for persisting an fusing large and numerous datasets and training AI models for production against those datasets.\n",
    "The Lucd platform is an end to end platform that can be deployed in public cloud environments, on premise on bare metal hardware, or the Lucd multi-tenant PaaS can be directly accessed.  The platform consists of:\n",
    "\n",
    " - A scalable open data ingest capability\n",
    " - A petabyte scale unified data space data repository\n",
    " - 3-D Visualization and Exploration\n",
    " - An Exploratory Data Analysis Rest Service\n",
    " - A Kubernetes environment to train PyTorch and TensorFlow models\n",
    " - NLP Word Embedding and Explainable AI Assets\n",
    " - Model results visualization and exporting to internal or external serving capability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/Architecture1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction, Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates the steps required to train an AI model on data leveraging the Lucd Data Science Platform.  The tutorial is a toy, leveraging the IRIS dataset, designed to show the basic steps to train a model.  In the example a Virtual Data Set is created, A custom operation adds a categorical feature to the existing continuous features.  Then a custom Pytorch model is developed and trained in the platform.  Both the Lucd 3D UI and the Lucd Python Client are leveraged during the tutorial.  The tutorial is brokein up into three Parts:\n",
    "1. Part 1: Creating a Virtual Data Set (VDS) https://github.com/jmstadt/Tutorials/blob/master/Lucd_Part_1_of_3_Data_Science_Tutorial.ipynb\n",
    "2. Part 2: Performing a Custom Operation during Exploratory Data Analysis https://github.com/jmstadt/Tutorials/blob/master/Lucd_Part_2_of_3_Data_Science_Tutorial.ipynb\n",
    "3. __Part 3: Developing a Custom AI Model and Training in the Lucd Platform__\n",
    "\n",
    "Prerequisites are:\n",
    " - Deploying the Lucd Platform if required (the Lucd Platform can be deployed as a dedicated on premise or cloud instance or the Lucd PaaS can be accessed.\n",
    "     - This step is outside of scope for this tutorial.  For Deploying the platform, refer to:\n",
    "         - Lucd Infrastructure Requirements:  https://community.lucd.ai/hc/en-us/articles/360037762592-Infrastructure-Requirements-v6-2-0\n",
    "         - Lucd Deployment Guide:  https://community.lucd.ai/hc/en-us/articles/360037762792-Deployment-Guide-v6-2-0\n",
    "         - Create Security and Access Framework, Create User Account(s) \n",
    "\n",
    " - Obtaining a Lucd account with appropriate security settings to access/retrieve data.  Contact marketing@lucd.ai or refer to:\n",
    "     - https://community.lucd.ai/hc/en-us/articles/360037995531\n",
    " - Obtaining the Lucd Python Client package (in the future pip install will be available).  For now, obtain by contacting marketing@lucd.ai\n",
    " - Downloading and installing a Jupyter notebook (this tutorial assumes that an Anaconda Jupyter notebook is used)\n",
    "     - Also refer to:  https://github.com/jmstadt/Tutorials/blob/master/Lucd_Pulling_a_Virtual_Data_Set_from_the_Lucd_Unified_Data_Space.ipynb\n",
    " - Setting up NiFi and Ingesting data is outside the scope of this tutorial.\n",
    "     - Refer to: https://community.lucd.ai/hc/en-us/articles/360038129271-NiFi-Configuration-v6-2-0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Complete Part 1 and Part 2 of this Lucd Data Science Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of Part 2, we have a dataframe loaded into our Lucd Python Client.  We are going to train a Pytorch Classification Model to predict Species types from the continuous variables (from the original dataset) and the categorical variable (that we created from the custom operation).\n",
    "\n",
    "For details of training an AI model in the Lucd platform refer to:  https://community.lucd.ai/hc/en-us/articles/360022454472-Lucd-Modeling-Framework-v6-1-3\n",
    "\n",
    "For this notebook, we will start by reloading the VDS we created at the end of Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lucd\n",
    "from eda.lib import lucd_uds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "login to the Lucd Enterprie AI Platform with the same credentials you used in Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = lucd.LucdClient(domain=\"<your domain>\",\n",
    "                         username=\"<your username>\",\n",
    "                         password=\"<your password>\",\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the VDS ID and pull into a dask dataframe as per the end of Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-22 14:32:00,439 | root | INFO | dask.py:28 | Creating Dask LocalCluster: http://localhost:60000/status\n",
      "c:\\users\\markstadtmueller\\anaconda3\\lib\\site-packages\\distributed\\dashboard\\core.py:79: UserWarning: \n",
      "Port 60000 is already in use. \n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the diagnostics dashboard on a random port instead.\n",
      "  warnings.warn(\"\\n\" + msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'demo_9223370449329390901': {'description': 'iris', 'model': {'data': ['flower.petal_length', 'flower.petal_width', 'flower.sepal_length', 'flower.sepal_width', 'flower.species', 'flower_mean', 'std.display', 'std.model', 'std.source', 'std.timestamp'], 'labels': ['flower.petal_length', 'flower.petal_width', 'flower.sepal_length', 'flower.sepal_width', 'flower.species', 'flower_mean', 'std.display', 'std.model', 'std.source', 'std.timestamp']}, 'name': 'april21_custom', 'operations': [{'command': 'custom', 'custom_operation_id': 'demo_9223370449329589395', 'dataset': '637231220528707971', 'orient': 'records', 'parameters': {'function_apply_method': 'map_partitions'}, 'return': '637231221401607846'}], 'query': {'dataset': '637231220528707971', 'query': {'bool': {'filter': [], 'must': [{'bool': {'should': [{'match_phrase': {'source': 'iris'}}]}}, {'range': {'content_date': {'gte': None, 'lt': None}}}], 'must_not': []}}, 'size': 100}, 'query_size': 150, 'username': 'demo'}}\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "ddf = lucd_uds.get_dataframe(\"demo_9223370449329390901\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can call .head() on the new dataframe, there is now your VDS with the \"flower_mean\" column per your custom operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. This section encodes the categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from eda.lib import lucd_uds\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(features, labels):\n",
    "    new_labels = labels.replace([\"I. versicolor\", \"I. setosa\", \"I. virginica\"], [2, 0, 1])\n",
    "    features['flower_mean'] = features['flower_mean'].replace([False, True], [\"False\", \"True\"])\n",
    "    return features, new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _label_mapping():\n",
    "    return {2: 'I. setosa', 1: 'I. virginica', 0: 'I. versicolor'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(num_features, training_steps, learning_rate, log_dir, training_data, evaluation_data):\n",
    "    type_dict = {\"flower.petal_length\": tf.float64,\n",
    "                 \"flower.petal_width\": tf.float64,\n",
    "                 \"flower.sepal_length\": tf.float64,\n",
    "                 \"flower.sepal_width\": tf.float64,\n",
    "                 \"flower_mean\": tf.string\n",
    "                 }\n",
    "\n",
    "    feature_labels = [\"flower.petal_length\", \"flower.petal_width\", \"flower.sepal_length\", \"flower.sepal_width\",\n",
    "                      \"flower_mean\"]\n",
    "\n",
    "    target_type = tf.int32\n",
    "\n",
    "    num_classes = 3\n",
    "\n",
    "    # Define the feature columns for inputs.\n",
    "    feature_columns = [\n",
    "        tf.feature_column.numeric_column(key=\"flower.petal_length\"),\n",
    "        tf.feature_column.numeric_column(key=\"flower.petal_width\"),\n",
    "        tf.feature_column.numeric_column(key=\"flower.sepal_length\"),\n",
    "        tf.feature_column.numeric_column(key=\"flower.sepal_width\"),\n",
    "        tf.feature_column.indicator_column(\n",
    "            tf.feature_column.categorical_column_with_vocabulary_list(key='flower_mean',\n",
    "                                                                      vocabulary_list=['True', 'False'])\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    serving_input_receiver_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
    "        tf.feature_column.make_parse_example_spec(feature_columns))\n",
    "\n",
    "    # Create the Estimator\n",
    "    training_config = tf.estimator.RunConfig(\n",
    "        save_summary_steps=10,\n",
    "        save_checkpoints_steps=10)\n",
    "\n",
    "    classifier = tf.estimator.DNNClassifier(\n",
    "        config=training_config,\n",
    "        feature_columns=feature_columns,\n",
    "        hidden_units=[10, 20, 10],\n",
    "        n_classes=num_classes,\n",
    "        model_dir=log_dir\n",
    "    )\n",
    "\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn=lambda: lucd_uds.get_tf_dataset(feature_labels, type_dict, training_data, num_features,\n",
    "                                                 target_type).repeat(count=None).shuffle(100).batch(100),\n",
    "        max_steps=training_steps)\n",
    "\n",
    "    latest_exporter = tf.estimator.LatestExporter(\n",
    "        name=\"models\",\n",
    "        serving_input_receiver_fn=serving_input_receiver_fn,\n",
    "        exports_to_keep=10)\n",
    "\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn=lambda: lucd_uds.get_tf_dataset(feature_labels, type_dict, evaluation_data, num_features,\n",
    "                                                 target_type).repeat(count=None).shuffle(100).batch(100),\n",
    "        exporters=latest_exporter)\n",
    "\n",
    "    return classifier, train_spec, eval_spec, feature_labels, type_dict, target_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_classification(_estimator, input_fn, mode):\n",
    "    \"\"\" Returns a list of predicted values using a TensorFlow estimator on a given TensorFlow DataSet.\n",
    "\n",
    "    Args:\n",
    "        _estimator: TensorFlow estimator to use for getting predictions.\n",
    "        input_fn: TensorFlow input function feeding DataSet with testing/hold-out data.\n",
    "        mode: String indicating type of classification done by the estimator: \"binary\" or \"multi.\"\n",
    "\n",
    "    Returns:\n",
    "        List of predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = _estimator.predict(input_fn=input_fn)\n",
    "\n",
    "    return_list = []\n",
    "\n",
    "    for pred in predictions:\n",
    "        if mode == \"binary\":\n",
    "            for key in pred:\n",
    "                # log.debug(f\"\\n\\n\\nKEY: {key}\")\n",
    "                prob_value = pred[key][0]\n",
    "                break\n",
    "\n",
    "            if prob_value > .5:\n",
    "                return_list.append(1)\n",
    "            else:\n",
    "                return_list.append(0)\n",
    "        elif mode == \"multiclass\":\n",
    "            for key in pred:\n",
    "                prob_value = pred[key]\n",
    "                break\n",
    "\n",
    "            return_list.append(np.argmax(prob_value))\n",
    "        elif mode == \"tf_premade_multiclass\":\n",
    "            return_list.append(pred[\"class_ids\"][0])\n",
    "\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    # Get required training parameters\n",
    "    tid = args['train_id']\n",
    "    vds = args['vds']\n",
    "\n",
    "    lr = args['parameters']['lr']\n",
    "    evaluation_dataset_percent = args['parameters']['eval_percent']\n",
    "    testing_dataset_percent = args['parameters']['test_percent']\n",
    "    training_steps = args['parameters']['steps']\n",
    "    save_path = args['exportdir']\n",
    "\n",
    "    delayed_values_training, delayed_values_evaluation, delayed_values_testing, my_df_testing_label, num_features =\\\n",
    "        lucd_uds.train_test_eval_split_tensorflow(vds,\n",
    "                                                  evaluation_dataset_percent,\n",
    "                                                  testing_dataset_percent,\n",
    "                                                  process_data)\n",
    "\n",
    "    print(f\"Train: {len(delayed_values_training)}, Test: {len(delayed_values_testing)}\")\n",
    "\n",
    "    _estimator, train_spec, eval_spec, feature_labels, type_dict, target_type = \\\n",
    "        model(num_features, training_steps, lr, save_path, delayed_values_training,\n",
    "              delayed_values_evaluation)\n",
    "\n",
    "    _tuple = tf.estimator.train_and_evaluate(_estimator, train_spec=train_spec, eval_spec=eval_spec)\n",
    "\n",
    "    print(_tuple)\n",
    "\n",
    "    # compute confusion matrix\n",
    "    mode = \"tf_premade_multiclass\"\n",
    "    predictions = get_predictions_classification(_estimator,\n",
    "                                                 lambda: lucd_uds.get_tf_dataset(feature_labels,\n",
    "                                                                                 type_dict,\n",
    "                                                                                 delayed_values_testing,\n",
    "                                                                                 num_features,\n",
    "                                                                                 target_type).batch(1),\n",
    "                                                 mode)\n",
    "\n",
    "    actual = (my_df_testing_label.compute()).iloc[:, 0].tolist()\n",
    "    for index, item in enumerate(actual):\n",
    "        if item == 'I. versicolor':\n",
    "            actual[index] = 0\n",
    "        elif item == 'I. virginica':\n",
    "            actual[index] = 1\n",
    "        else:\n",
    "            actual[index] = 2\n",
    "\n",
    "    label_mapping = _label_mapping()\n",
    "\n",
    "    cm_out = tf.math.confusion_matrix(actual, predictions)\n",
    "    cm_out = cm_out.numpy()\n",
    "    print(cm_out)\n",
    "    cm_list = []\n",
    "    i = 0\n",
    "    for r in cm_out:\n",
    "        j = 0\n",
    "        for c in r:\n",
    "            i_string = label_mapping[i]\n",
    "            j_string = label_mapping[j]\n",
    "            cm_list.append(i_string + ':' + j_string + ':' + str(cm_out[i, j]))\n",
    "            j += 1\n",
    "        i += 1\n",
    "    cm_string = ';'.join(cm_list)\n",
    "    print(cm_string)\n",
    "\n",
    "    # compute average precision, recall, and f1 score for multi-class (not multi-label)\n",
    "    actual_np = np.array(actual)\n",
    "    predictions_np = np.array(predictions)\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(actual_np, predictions_np,\n",
    "                                                                                 average='macro')\n",
    "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(actual_np, predictions_np,\n",
    "                                                                                 average='micro')\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(actual_np, predictions_np,\n",
    "                                                                                          average='weighted')\n",
    "\n",
    "    # compute precision, recall, and f1 score per label\n",
    "    labels = list(set(actual))\n",
    "    results = precision_recall_fscore_support(actual_np, predictions_np, average=None, labels=labels)\n",
    "    results_string = ''\n",
    "    for i in range(0, len(labels)):\n",
    "        stat_list = []\n",
    "        for stat in results[i]:\n",
    "            stat_list.append(str(stat))\n",
    "        stat_string = ','.join(stat_list)\n",
    "        raw_label = labels[i]\n",
    "        results_string += (label_mapping[raw_label] + '-')\n",
    "        results_string += (stat_string + ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-22 15:04:08,916 | root | INFO | dask.py:28 | Creating Dask LocalCluster: http://localhost:60000/status\n",
      "c:\\users\\markstadtmueller\\anaconda3\\lib\\site-packages\\distributed\\dashboard\\core.py:79: UserWarning: \n",
      "Port 60000 is already in use. \n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the diagnostics dashboard on a random port instead.\n",
      "  warnings.warn(\"\\n\" + msg)\n",
      "2020-04-22 15:04:25,940 | tensorflow | INFO | estimator.py:216 | Using config: {'_model_dir': '.', '_tf_random_seed': None, '_save_summary_steps': 10, '_save_checkpoints_steps': 10, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "2020-04-22 15:04:25,941 | tensorflow | INFO | estimator_training.py:186 | Not using Distribute Coordinator.\n",
      "2020-04-22 15:04:25,942 | tensorflow | INFO | training.py:612 | Running training and evaluation locally (non-distributed).\n",
      "2020-04-22 15:04:25,943 | tensorflow | INFO | training.py:700 | Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 10 or save_checkpoints_secs None.\n",
      "2020-04-22 15:04:25,960 | tensorflow | INFO | estimator.py:367 | Skipping training since max_steps has already saved.\n",
      "2020-04-22 15:04:25,990 | tensorflow | WARNING | deprecation.py:506 | From c:\\users\\markstadtmueller\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'demo_9223370449524874138': {'description': 'Tutorial', 'model': {'data': ['flower.petal_length', 'flower.petal_width', 'flower.sepal_length', 'flower.sepal_width', 'flower_mean'], 'labels': ['flower.species']}, 'name': 'My_IRIS_VDS_1', 'operations': [{'command': 'custom', 'custom_operation_id': 'demo_9223370449703715244', 'dataset': '637229128990491299', 'orient': 'records', 'parameters': {'function_apply_method': 'map_partitions'}, 'return': '637229265635208812'}], 'query': {'aggs': {'agg_source': {'aggs': {'agg_model': {'aggs': {'topHits': {'top_hits': {'size': 10}}}, 'terms': {'field': 'model'}}}, 'terms': {'field': 'source'}}}, 'dataset': '637229128990491299', 'query': {'function_score': {'functions': [{'random_score': {}}], 'query': {'bool': {'filter': [], 'must': [{'bool': {'should': [{'match_phrase': {'source': 'iris'}}]}}, {'range': {'content_date': {'gte': None, 'lt': None}}}], 'must_not': []}}}}, 'size': 100}, 'query_size': 150, 'username': 'demo'}}\n",
      "()\n",
      "Train: 1, Test: 1\n",
      "(None, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-22 15:04:26,079 | tensorflow | INFO | estimator.py:1151 | Calling model_fn.\n",
      "2020-04-22 15:04:26,101 | tensorflow | WARNING | base_layer.py:1790 | Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2020-04-22 15:04:26,535 | tensorflow | WARNING | deprecation.py:323 | From c:\\users\\markstadtmueller\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\feature_column\\feature_column_v2.py:4267: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "2020-04-22 15:04:26,535 | tensorflow | WARNING | deprecation.py:323 | From c:\\users\\markstadtmueller\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\feature_column\\feature_column_v2.py:4322: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "2020-04-22 15:04:26,843 | tensorflow | INFO | estimator.py:1153 | Done calling model_fn.\n",
      "2020-04-22 15:04:27,066 | tensorflow | INFO | monitored_session.py:246 | Graph was finalized.\n",
      "2020-04-22 15:04:27,083 | tensorflow | INFO | saver.py:1284 | Restoring parameters from .\\model.ckpt-10000\n",
      "2020-04-22 15:04:27,160 | tensorflow | INFO | session_manager.py:504 | Running local_init_op.\n",
      "2020-04-22 15:04:27,171 | tensorflow | INFO | session_manager.py:507 | Done running local_init_op.\n",
      "c:\\users\\markstadtmueller\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "2020-04-22 15:04:28,142 | root | INFO | <ipython-input-8-a83b4267b074>:23 | Model Training Complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0 11  0]\n",
      " [ 0 15  0]\n",
      " [ 0  7  0]]\n",
      "I. versicolor:I. versicolor:0;I. versicolor:I. virginica:11;I. versicolor:I. setosa:0;I. virginica:I. versicolor:0;I. virginica:I. virginica:15;I. virginica:I. setosa:0;I. setosa:I. versicolor:0;I. setosa:I. virginica:7;I. setosa:I. setosa:0\n"
     ]
    }
   ],
   "source": [
    "from lucd import LucdClient, log\n",
    "\n",
    "# Use this for local EDA testing / development\n",
    "username = '<your username>'\n",
    "client = LucdClient(domain='<your domain>',\n",
    "                    username=username,\n",
    "                    password='<your password>')\n",
    "\n",
    "client.set_max_data(limit=1000)\n",
    "\n",
    "main({\n",
    "    'train_id': '<your_local_name_for_reference>',\n",
    "    'vds': '<your vds id from above>',\n",
    "    'exportdir': '.',\n",
    "    'parameters': {\n",
    "        'lr': 0.01,\n",
    "        'eval_percent': 0.10,\n",
    "        'test_percent': 0.10,\n",
    "        'steps': 100\n",
    "    }\n",
    "})\n",
    "\n",
    "log.info(\"Model Training Complete.\")\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Sections 2 through 5 as a .py file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download the notebook as .py file and trim out, the other sections and save for uploading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Model into Lucd Platform and Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to the modeling tab in the platform and upload the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"images/Upload_Model.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter in model information and click the green check mark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"images/Model_Info.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hit refresh and select your model by clicking on it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"images/Start_Training.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the Training pop up, Select your VDS, and Enter your Training Parameters and hit the green check mark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"images/VDS_and_Training_Parameters.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see your model with a yellow dot next to it, indicating that training has started, the dot will go green when training is complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"images/Training_Started.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the upper left pulldown menu, select trained models.  When your model training is complete, you will see your model and if you click on it, on the right hand side you will see the training results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"images/Training_Complete.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the lower right hand side you can select the download button and export your trained model i.e. \"Download Artifacts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ends the tutorial.  This tutorial walked through creating a VDS, performing custom operations, creating and running an AI model and exporting that model.  Many more capabilities are in the platform and can be explored individually or via additional tutorials, articles, or user guides.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
